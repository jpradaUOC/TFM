{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************************************************************************<br>\n",
    "********* ENTRENAMIENTO CHATBOT BERT EXPRESSITO FULL POWER *******************<br>\n",
    "******************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************************** IMPORTACIONES *******************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras as k\n",
    "import transformers as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, auc, roc_curve\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "********************** VARIABLE GLOBAL Y CARGA DE DATOS **************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-11 10:16:36.217738: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-01-11 10:16:36.217784: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (gpu): /proc/driver/nvidia/version does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable global para determinar el path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/iexpress/TCGA/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-d7ba8573d73dfdbb\n",
      "Reusing dataset csv (/home/iexpress/.cache/huggingface/datasets/csv/default-d7ba8573d73dfdbb/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac59dacfb9f4ddb8d848c559c72ac6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-bb82920b8a7fea9d\n",
      "Reusing dataset csv (/home/iexpress/.cache/huggingface/datasets/csv/default-bb82920b8a7fea9d/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180467c993984febb8a448b8f218ee0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a9ce4f04f913ec8c\n",
      "Reusing dataset csv (/home/iexpress/.cache/huggingface/datasets/csv/default-a9ce4f04f913ec8c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "523d0ea646fe43debf21fb8ee1a4d71e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(os.path.join(path_tabular, 'clean_data.csv'), sep = ';')\n",
    "selected_columns = [\"|\" not in i for i in tab_dat.columns]\n",
    "data = tab_dat[tab_dat.columns[selected_columns]]\n",
    "data_train = data[data.dataset.values == 'train']\n",
    "data_val = data[data.dataset.values == 'val']\n",
    "data_test = data[data.dataset.values == 'test']\n",
    "y_train = data_train['target']\n",
    "y_val = data_val['target']\n",
    "y_test = data_test['target']\n",
    "del data_train['dataset']\n",
    "del data_val['dataset']\n",
    "del data_test['dataset']\n",
    "del data_train['target']\n",
    "del data_val['target']\n",
    "del data_test['target']\n",
    "del data_train['index']\n",
    "del data_val['index']\n",
    "del data_test['index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se carga la lista de targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = pd.read_csv(path + \"/topic_levels.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Cargamos el checkpoint de Full Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-04 22:17:38.714269: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=len(target_list))\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************************ TOKENIZACION *********************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Creamos una funci칩n para tokenizar el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(data):\n",
    "    return tokenizer(data[\"input\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Tratamos los datos del tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/iexpress/.cache/huggingface/datasets/csv/default-d7ba8573d73dfdbb/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-ede567f04c0e4009.arrow\n",
      "Loading cached shuffled indices for dataset at /home/iexpress/.cache/huggingface/datasets/csv/default-d7ba8573d73dfdbb/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-8b2f5c8bb81c9afe.arrow\n",
      "Loading cached processed dataset at /home/iexpress/.cache/huggingface/datasets/csv/default-bb82920b8a7fea9d/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-f1709c15b2a1eaaa.arrow\n",
      "Loading cached shuffled indices for dataset at /home/iexpress/.cache/huggingface/datasets/csv/default-bb82920b8a7fea9d/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-ccb4a443dd633f25.arrow\n",
      "Loading cached processed dataset at /home/iexpress/.cache/huggingface/datasets/csv/default-a9ce4f04f913ec8c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-a99e65635aab6478.arrow\n",
      "Loading cached shuffled indices for dataset at /home/iexpress/.cache/huggingface/datasets/csv/default-a9ce4f04f913ec8c/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-7910c8dda3e85b49.arrow\n"
     ]
    }
   ],
   "source": [
    "dat_tokenize_train = data_train.map(tokenize_function, batched=True).shuffle(seed=7).remove_columns([\"input\"]).with_format(\"tensorflow\")\n",
    "dat_tokenize_val = data_val.map(tokenize_function, batched=True).shuffle(seed=7).remove_columns([\"input\"]).with_format(\"tensorflow\")\n",
    "dat_tokenize_test = data_test.map(tokenize_function, batched=True).shuffle(seed=7).remove_columns([\"input\"]).with_format(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Obtenemos las features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = {x: dat_tokenize_train[\"train\"][x] for x in tokenizer.model_input_names}\n",
    "val_features = {x: dat_tokenize_val[\"train\"][x] for x in tokenizer.model_input_names}\n",
    "test_features = {x: dat_tokenize_test[\"train\"][x] for x in tokenizer.model_input_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_tf_train = tf.data.Dataset.from_tensor_slices((train_features, dat_tokenize_train[\"train\"][\"target\"])).batch(batch_size)\n",
    "dat_tf_val = tf.data.Dataset.from_tensor_slices((val_features, dat_tokenize_val[\"train\"][\"target\"])).batch(batch_size)\n",
    "dat_tf_test = tf.data.Dataset.from_tensor_slices((test_features, dat_tokenize_test[\"train\"][\"target\"])).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************** ENTRENAMIENTO MODELO *********************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Usamos Keras para compilar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=tf.metrics.SparseCategoricalAccuracy())\n",
    "#model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Visualizamos su estructura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  177853440 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  13073     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 177,866,513\n",
      "Trainable params: 177,866,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fijamos un callback para generar un early_stopping y guardar el mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Entrenamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "661/661 [==============================] - 626s 927ms/step - loss: 1.4210 - sparse_categorical_accuracy: 0.5935 - val_loss: 0.8846 - val_sparse_categorical_accuracy: 0.7576\n",
      "Epoch 2/10\n",
      "661/661 [==============================] - 612s 926ms/step - loss: 0.5770 - sparse_categorical_accuracy: 0.8441 - val_loss: 0.9667 - val_sparse_categorical_accuracy: 0.7374\n",
      "Epoch 3/10\n",
      "661/661 [==============================] - 612s 927ms/step - loss: 0.3682 - sparse_categorical_accuracy: 0.8965 - val_loss: 0.6206 - val_sparse_categorical_accuracy: 0.8249\n",
      "Epoch 4/10\n",
      "661/661 [==============================] - 613s 927ms/step - loss: 0.2755 - sparse_categorical_accuracy: 0.9235 - val_loss: 0.5177 - val_sparse_categorical_accuracy: 0.8653\n",
      "Epoch 5/10\n",
      "661/661 [==============================] - 613s 928ms/step - loss: 0.2339 - sparse_categorical_accuracy: 0.9366 - val_loss: 0.5012 - val_sparse_categorical_accuracy: 0.8754\n",
      "Epoch 6/10\n",
      "661/661 [==============================] - 612s 926ms/step - loss: 0.1928 - sparse_categorical_accuracy: 0.9461 - val_loss: 0.5953 - val_sparse_categorical_accuracy: 0.8485\n",
      "Epoch 7/10\n",
      "661/661 [==============================] - 612s 926ms/step - loss: 0.1653 - sparse_categorical_accuracy: 0.9514 - val_loss: 0.8703 - val_sparse_categorical_accuracy: 0.8081\n"
     ]
    }
   ],
   "source": [
    "model_history = model.fit(dat_tf_train, validation_data=dat_tf_test, epochs=10, batch_size=batch_size, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Guardamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"model_saved_bert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************************* VALIDACION *********************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Predecimos el set de validaci칩n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = model.predict(dat_tf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Multilabel classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test_probs = tf.nn.softmax(predictions_test[\"logits\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Obtenemos el valor m치ximo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test_classes = np.argmax(predictions_test_probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************************* EVALUACION *********************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Evaluamos el modelo(Precision, Recall, F1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_model = classification_report(y_true = dat_tokenize_test[\"train\"][\"target\"],\n",
    "                                         y_pred = predictions_test_classes,\n",
    "                                         target_names = target_list.topic,\n",
    "                                        output_dict = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se guardan las evaluaciones del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluaciones_modelo = pd.DataFrame(evaluation_model).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluaciones_modelo.to_csv(path + \"/data_evaluation_full_bert.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluaciones_modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************* MATRIZ DE CONFUSION *********************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13  0  0  1  0  0  1  0  0  1  1  0  0  0  0  0  0]\n",
      " [ 0 17  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0]\n",
      " [ 0  1 15  0  1  0  0  0  0  0  1  0  0  0  0  0  0]\n",
      " [ 0  1  0 12  0  0  0  0  0  1  1  0  2  0  0  0  0]\n",
      " [ 0  0  0  0 17  0  0  0  0  0  0  0  0  0  0  1  0]\n",
      " [ 0  0  0  0  0 14  0  0  0  0  1  0  0  0  0  2  0]\n",
      " [ 0  1  0  0  0  0 16  0  0  0  0  0  0  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0 14  0  0  1  0  2  0  0  0  0]\n",
      " [ 0  1  0  0  0  1  0  0 14  0  0  0  0  0  1  0  0]\n",
      " [ 0  1  0  2  0  0  0  0  0 14  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 18  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  0  0 14  2  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  1  0  1 15  0  0  0  0]\n",
      " [ 1  0  1  0  0  0  0  0  0  0  0  0  0  5  0 11  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0 16  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  1  0 16  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  6 10]] \n",
      "\n",
      "[[0.765 0.    0.    0.059 0.    0.    0.059 0.    0.    0.059 0.059 0.\n",
      "  0.    0.    0.    0.    0.   ]\n",
      " [0.    0.944 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.056 0.   ]\n",
      " [0.    0.056 0.833 0.    0.056 0.    0.    0.    0.    0.    0.056 0.\n",
      "  0.    0.    0.    0.    0.   ]\n",
      " [0.    0.059 0.    0.706 0.    0.    0.    0.    0.    0.059 0.059 0.\n",
      "  0.118 0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.944 0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.056 0.   ]\n",
      " [0.    0.    0.    0.    0.    0.824 0.    0.    0.    0.    0.059 0.\n",
      "  0.    0.    0.    0.118 0.   ]\n",
      " [0.    0.056 0.    0.    0.    0.    0.889 0.    0.    0.    0.    0.\n",
      "  0.    0.    0.    0.056 0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.824 0.    0.    0.059 0.\n",
      "  0.118 0.    0.    0.    0.   ]\n",
      " [0.    0.059 0.    0.    0.    0.059 0.    0.    0.824 0.    0.    0.\n",
      "  0.    0.    0.059 0.    0.   ]\n",
      " [0.    0.059 0.    0.118 0.    0.    0.    0.    0.    0.824 0.    0.\n",
      "  0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.\n",
      "  0.    0.    0.    0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.056 0.    0.    0.    0.778\n",
      "  0.111 0.    0.    0.056 0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.059 0.    0.059\n",
      "  0.882 0.    0.    0.    0.   ]\n",
      " [0.056 0.    0.056 0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.278 0.    0.611 0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.056 0.    0.    0.    0.\n",
      "  0.    0.    0.889 0.056 0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.059 0.    0.941 0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.059 0.353 0.588]]\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "## Calculamos la matriz de confusi칩n\n",
    "cm = confusion_matrix(dat_tokenize_test[\"train\"][\"target\"], predictions_test_classes)\n",
    "print(cm, \"\\n\")\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print(cm.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cm).to_csv(path + \"/confusion_matrix_full_bert.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
