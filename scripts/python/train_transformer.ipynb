{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping, ReduceLROnPlateau, Callback, LearningRateScheduler\n",
    "import transformers as t\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, auc, roc_curve\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, plot_confusion_matrix, roc_curve\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "input_shape = (224, 224, 3)\n",
    "seed = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tabular = '/home/iexpress/TCGA/data/'\n",
    "path_images = '/home/iexpress/TCGA/data/images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_dat = pd.read_csv(os.path.join(path_tabular, 'clean_data.csv'), sep = ';')\n",
    "selected_columns = [\"|\" not in i for i in tab_dat.columns]\n",
    "tab_dat = tab_dat[tab_dat.columns[selected_columns]]\n",
    "tab_X_train = tab_dat[tab_dat.dataset.values == 'train']\n",
    "tab_X_val = tab_dat[tab_dat.dataset.values == 'val']\n",
    "tab_X_test = tab_dat[tab_dat.dataset.values == 'test']\n",
    "y_train = tab_X_train['target']\n",
    "y_val = tab_X_val['target']\n",
    "y_test = tab_X_test['target']\n",
    "del tab_X_train['dataset']\n",
    "del tab_X_val['dataset']\n",
    "del tab_X_test['dataset']\n",
    "del tab_X_train['target']\n",
    "del tab_X_val['target']\n",
    "del tab_X_test['target']\n",
    "del tab_X_train['index']\n",
    "del tab_X_val['index']\n",
    "del tab_X_test['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pickle.load(open(os.path.join(path_tabular, 'X_train'), \"rb\"))\n",
    "X_val = pickle.load(open(os.path.join(path_tabular, 'X_val'), \"rb\"))\n",
    "X_test = pickle.load(open(os.path.join(path_tabular, 'X_test'), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train tab data size = ' + str(tab_X_train.shape))\n",
    "print('Train image data size = ' + str(X_train.shape))\n",
    "print('Train target size = ' + str(y_train.shape))\n",
    "print('Validation tab data size = ' + str(tab_X_val.shape))\n",
    "print('Validation image data size = ' + str(X_val.shape))\n",
    "print('Validation target size = ' + str(y_val.shape))\n",
    "print('Test tab data size = ' + str(tab_X_test.shape))\n",
    "print('Test target size = ' + str(y_test.shape))\n",
    "print('Test image data size = ' + str(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "decay = 0.9\n",
    "batch_size = 256\n",
    "num_epochs = 100\n",
    "image_size = 72  # We'll resize input images to this size\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grafica_entrenamiento(tr_auc, val_auc, tr_loss, val_loss, best_i,\n",
    "                          figsize=(10,5), path_results = None):\n",
    "    plt.figure(figsize=figsize)\n",
    "    ax = plt.subplot(1,2,1)\n",
    "    plt.plot(1+np.arange(len(tr_loss)), np.array(tr_loss))\n",
    "    plt.plot(1+np.arange(len(val_loss)), np.array(val_loss))\n",
    "    plt.plot(1+best_i, val_loss[best_i], 'or')\n",
    "    plt.title('loss del modelo', fontsize=18)\n",
    "    plt.ylabel('loss', fontsize=12)\n",
    "    plt.xlabel('época', fontsize=18)        \n",
    "    plt.legend(['entrenamiento', 'validación'], loc='upper left')\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    \n",
    "    plt.plot(1+np.arange(len(tr_auc)),  np.array(tr_auc))\n",
    "    plt.plot(1+np.arange(len(val_auc)), np.array(val_auc))\n",
    "    plt.plot(1+best_i, val_auc[best_i], 'or')\n",
    "    plt.title('AUC', fontsize=18)\n",
    "    plt.ylabel('AUC', fontsize=12)\n",
    "    plt.xlabel('época', fontsize=18)    \n",
    "    plt.legend(['entrenamiento', 'validación'], loc='upper left')\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    if (path_results != None):\n",
    "        plt.savefig(os.path.join(path_results, 'auc_loss.png'))\n",
    "    plt.show()\n",
    "    \n",
    "class TrainingPlot(Callback):\n",
    "    \n",
    "    # This function is called when the training begins\n",
    "    def on_train_begin(self, logs={}):\n",
    "        # Initialize the lists for holding the logs, losses and accuracies\n",
    "        self.losses = []\n",
    "        self.auc = []\n",
    "        self.val_losses = []\n",
    "        self.val_auc = []\n",
    "        self.logs = []\n",
    "    \n",
    "    # This function is called at the end of each epoch\n",
    "    def on_epoch_end(self, epoch, logs={}):       \n",
    "       \n",
    "        \n",
    "        # Before plotting ensure at least 10 epochs have passed\n",
    "        if epoch > 2:\n",
    "             # Append the logs, losses and accuracies to the lists\n",
    "            self.logs.append(logs)        \n",
    "            self.auc.append(logs.get('auc'))        \n",
    "            self.val_auc.append(logs.get('val_auc'))\n",
    "            self.losses.append(logs.get('loss'))\n",
    "            self.val_losses.append(logs.get('val_loss'))\n",
    "            best_i = np.argmax(self.val_auc)\n",
    "            grafica_entrenamiento(self.auc, self.val_auc, self.losses, self.val_losses, best_i)\n",
    "\n",
    "plot_losses = TrainingPlot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class print_learning_rate(Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        lr = self.model.optimizer.lr\n",
    "        print(f'Learning rate = {K.eval(lr):.5f}')\n",
    "print_lr = print_learning_rate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "    \n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        #layers.RandomFlip(\"horizontal\"),\n",
    "        #layers.RandomRotation(factor=0.02),\n",
    "        #layers.RandomZoom(\n",
    "        #    height_factor=0.2, width_factor=0.2\n",
    "        #),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(1, activation='sigmoid')(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model, X_train, y_train, X_val, y_val, class_weights = 0.5):\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    ),\n",
    "    metrics=[metrics.BinaryAccuracy(name = \"acc\"),\n",
    "            metrics.AUC(name = \"auc\")])\n",
    "\n",
    "#     checkpoint_filepath = \"/home/iexpress/TCGA/checkpoints/transformer\"\n",
    "#     checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "#         checkpoint_filepath,\n",
    "#         monitor=\"val_accuracy\",\n",
    "#         save_best_only=True,\n",
    "#         save_weights_only=True,\n",
    "#     )\n",
    "    \n",
    "    def lr_time_based_decay(epoch, lr):\n",
    "        return lr * decay\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_auc', patience=10, verbose=1, mode='max', restore_best_weights=True)\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_auc', factor=0.1, patience=3, mode='min')    \n",
    "\n",
    "    lr_decay = LearningRateScheduler(lr_time_based_decay, verbose=1)\n",
    "\n",
    "    history = model.fit(\n",
    "        x=X_train,\n",
    "        y=y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        class_weight=class_weights,\n",
    "        callbacks=[early_stopping,  lr_decay, plot_losses, print_lr],\n",
    "        verbose = 1,\n",
    "    )\n",
    "\n",
    "    # model.load_weights(checkpoint_filepath)\n",
    "    \n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight(class_weight = 'balanced',\n",
    "                                                 classes = np.unique(y_train),\n",
    "                                                 y = y_train)\n",
    "class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_vit_classifier()\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history, model = run_experiment(model, \n",
    "                                X_train, y_train, \n",
    "                                X_val, y_val, \n",
    "                                class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train)\n",
    "pred_val = model.predict(X_val)\n",
    "pred_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_train = roc_auc_score(y_true = y_train, y_score = pred_train)\n",
    "auc_val = roc_auc_score(y_true = y_val, y_score = pred_val)\n",
    "auc_test = roc_auc_score(y_true = y_test, y_score = pred_test)\n",
    "print('AUC train = %s - AUC val = %s - AUC test = %s' % (str(auc_train), str(auc_val), str(auc_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
